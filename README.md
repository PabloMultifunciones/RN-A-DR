# RN-A-DR
Redes Neuronales - Adicional - Porque usar Discount Rewards
### Introduccion ###

Aunque las tasas de descuento son una parte integral de los problemas de decisi√≥n de Markov y el aprendizaje por refuerzo (RL), a menudo seleccionamos Œ≥=0.9 o Œ≥=0.99 sin pensarlo dos veces. Seguramente, cuando se nos pregunta, tenemos algunas intuiciones como 'las recompensas de hoy valen m√°s que las recompensas de ma√±ana' o 'compensar la incertidumbre'. Cuando se le presiona, ¬øpuede defender por qu√© se mantienen esas intuiciones? ¬øPor qu√© eliges Œ≥=0,8 en lugar de Œ≥=0,9? ¬øNo est√° ya incorporada la incertidumbre en el valor esperado? Si no tiene una respuesta instant√°nea lista, este art√≠culo puede arrojar algo de luz sobre el asunto.  

### Descuento en matem√°ticas ###

Desde una perspectiva estrictamente matem√°tica, el prop√≥sito de una tasa de descuento es obvio, al menos para problemas de horizonte infinito. De la ecuaci√≥n recursiva de Bellman aprendemos a resolver funciones de valor para una secuencia de estados:  

![1_WwN-sAkWsHTEh8ECNJE6KA](https://user-images.githubusercontent.com/95035101/200392463-f8449335-1520-41e6-a858-92570a27d5ec.png)  

Si esa secuencia es infinita, tambi√©n lo es la serie de recompensas. Considere la siguiente secuencia de recompensas acumulativas G_t:  

G_t = R_t + R_t+1 + R_t+2 + ‚Ä¶ = 1 + 1 + 1 + ‚Ä¶ = ‚àû  

Como todos sabemos, sumar una serie de recompensas infinitas produce recompensas infinitas, lo que hace que el sistema de ecuaciones sea irresoluble. Afortunadamente, al sumar una tasa de descuento Œ≥ ‚àà [0,1) se obtiene una serie geom√©trica convergente. Por ejemplo, si ponemos Œ≥=0.8 obtenemos:  

G_t = Œ≥‚Å∞R_t + Œ≥¬πR_t+1 + Œ≥¬≤R_t+2 + ‚Ä¶ = 1 + 0,8 + 0,64 + ‚Ä¶ = 5  

Con este truco, podemos asignar valores a los estados y resolver el sistema de ecuaciones de Bellman. Por supuesto, en Aprendizaje por Refuerzo esa soluci√≥n ser√≠a aproximada.  

Eso explica el caso infinito, pero ¬øpor qu√© molestarse en descontar para horizontes de tiempo finitos? Podr√≠a argumentar que compensamos la incertidumbre, pero ¬øeso no se refleja ya en el valor esperado (multiplicando las recompensas futuras por sus probabilidades)? La perspectiva matem√°tica no resuelve esto, necesitamos sumergirnos un poco en la psique humana.  

### ¬øQu√© define mejor la psique humana que el dinero? ###

Un componente importante de la inversi√≥n es la existencia de una tasa libre de riesgo. Este es el rendimiento que se puede obtener sin ninguna incertidumbre o riesgo de impago, y sirve como referencia para todos los dem√°s rendimientos. Las letras del Tesoro de EE. UU. se utilizan a menudo como un proxy. Ponga un d√≥lar en una letra del Tesoro de EE. UU. al 2% y recibir√° $ 1.02 garantizados dentro de un a√±o. En consecuencia, preferimos $1 hoy a $1 el pr√≥ximo a√±o. Sin esfuerzo, podemos aumentar nuestra riqueza en un 2 % anual y, como tal, descontaremos las recompensas futuras en un 2 % para reflejar el valor del tiempo.  

Se vuelve m√°s interesante cuando se consideran instrumentos de riesgo como las acciones. Supongamos que una acci√≥n aumentar√° un 0% o un 4% dentro de un a√±o, ambas con una probabilidad de 0,5. El pago esperado es del 2%. Sin embargo, la posibilidad de terminar con las manos vac√≠as es sustancial. El inversionista t√≠pico preferir√° el bono del 2% libre de riesgo en este caso, a pesar de que los pagos esperados sean equivalentes. De ello se deduce que los rendimientos de las acciones se descuentan a una tasa mayor que los rendimientos de los bonos.  

Este fen√≥meno se conoce como aversi√≥n al riesgo. Las personas esperan ser compensadas por la incertidumbre, de lo contrario elegir√≠an la alternativa m√°s segura. A mayor incertidumbre, mayor tasa de descuento. Tal vez seleccionar√≠amos la acci√≥n si rindiera un 10 % en lugar de un 4 % (aumentando el pago esperado al 5 %). La inversi√≥n debe proporcionar una cierta prima de riesgo adem√°s de compensar el valor del tiempo.  

Todav√≠a hay muchos temas que quedan intactos, como la tendencia a utilizar el descuento exponencial (similar a RL), los costos de oportunidad o de arrepentimiento (solo puede invertir su dinero una vez), el comportamiento de b√∫squeda de riesgos (las loter√≠as tienen poco sentido desde la perspectiva de un inversor racional ), y casos l√≠mite extra√±amente inconsistentes. Por ahora, fij√©monos en la l√≥gica de que las tasas de descuento reflejan tanto el valor del tiempo como una prima de riesgo.  
### Rebajando en la vida ‚è≥ ###

El comportamiento de descuento no se limita a los d√≥lares. En la vida diaria, equilibramos constantemente la gratificaci√≥n a corto plazo y las consecuencias a largo plazo, compensamos la certeza y la incertidumbre. Acu√©state tarde y ma√±ana estar√°s cansado. Coma abundantemente durante el invierno y necesitar√° recortar grasa para lucir un cuerpo de playa en verano. Estudie d√≠a y noche ahora y, con suerte, coseche los frutos m√°s adelante.  

![1_Jlo_l2_0yx_EoQdgbdPihA](https://user-images.githubusercontent.com/95035101/200392790-8eeca46d-733f-4902-a597-760c6ee0553a.png)

Se han realizado innumerables estudios psicol√≥gicos sobre el tema, lo que sugiere que los humanos tienden a realizar algo cercano al descuento hiperb√≥lico en su toma de decisiones. Las personas que lucen un tatuaje de Carpe Diem en el brazo probablemente descuentan las recompensas futuras con bastante fuerza, otros podr√≠an darles m√°s peso. A pesar de las discrepancias entre los individuos, todos descontamos hasta cierto punto.   

Una raz√≥n muy natural de este fen√≥meno es la tasa de riesgo: la probabilidad de que no estemos vivos ma√±ana para cosechar recompensas. Aunque el riesgo de mortalidad aguda no es tan alto como el de nuestros antepasados ‚Äã‚Äãcazadores-recolectores, el impulso biol√≥gico de preferir las recompensas ahora permanece muy intacto. El peligro no necesita ser tan morboso como la muerte. Una carrera atl√©tica puede verse truncada por una lesi√≥n en la rodilla, y ese viaje de mochilero por Asia podr√≠a no ser posible dentro de cinco a√±os. Incluso en el nivel m√°s banal, preferimos tener una galleta ahora que una galleta la pr√≥xima semana.  

Como humanos, simplemente tenemos una predisposici√≥n programada para valorar las recompensas ahora m√°s que las del futuro (lejano), sean sensatas o no.   

### Descuento en Aprendizaje por Refuerzo üìñ ###

Ahora tenemos algunas ideas sobre la l√≥gica humana para el descuento, pero ¬øese razonamiento es v√°lido para los problemas de aprendizaje por refuerzo? A pesar de algunas conexiones sueltas entre las redes neuronales y el cerebro humano, normalmente los algoritmos de RL no est√°n dise√±ados para imitar el comportamiento humano. La tasa de riesgo tambi√©n es una raz√≥n susceptible, ya que podemos modelar los peligros en el medio ambiente. Por ejemplo, un juego de caminar por un acantilado termina cuando el agente se sube al acantilado; no necesitamos descontar m√°s para reflejar ataques card√≠acos o dedos golpeados. Un contraargumento ser√≠a que tales peligros podr√≠an ocurrir en entornos para los cuales la p√≥liza no est√° capacitada, utilizando la tasa de descuento como una especie de dispositivo de solidez.  

Para muchos problemas de RL, es perfectamente aceptable no descontar las recompensas futuras. Aun as√≠, existen razones v√°lidas para descontar en RL, incluso para horizontes finitos. Uno de ellos es el impacto real que las decisiones tienen sobre el desempe√±o a largo plazo. En √∫ltima instancia, depende del modelador qu√© tasa de descuento refleje mejor las recompensas acumuladas dentro del contexto del problema.

Supongamos que tengo que elegir entre una cena ligera o pesada esta noche. La decisi√≥n podr√≠a afectar mi sesi√≥n de gimnasio despu√©s, pero probablemente no tenga ning√∫n impacto en mi ascenso el pr√≥ximo a√±o. Aqu√≠, vemos una raz√≥n clara para descartar recompensas futuras: algunas consecuencias dif√≠cilmente pueden vincularse a la acci√≥n de hoy. El prop√≥sito de RL es incorporar los efectos posteriores de las decisiones actuales, pero las recompensas futuras pueden no estar correlacionadas. En general, cuanto m√°s estoc√°stico sea el entorno, menos impacto duradero tendr√°n nuestras acciones en el rendimiento.

Una respuesta en StackExchange formaliza esa noci√≥n de una manera interesante, expresando un par√°metro œÑ que refleja el intervalo de tiempo que nos interesa:

![Uploading 1_1LuXuLAZapiULzAQn2U71Q.png‚Ä¶]()

Como antes, suponga que la recompensa es siempre 1. Con Œ≥=0.8, la serie converge a 5. Efectivamente, las recompensas m√°s all√° de cinco pasos de tiempo hacia adelante (observe e^(-1/5)‚âà0.8) tienen poco impacto. De manera similar, una serie con Œ≥=0.9 converge a 10 y con Œ≥=0.99 converge a 100. Eso s√≠: una recompensa repentina de +100 despu√©s de t+œÑ a√∫n impacta sustancialmente la recompensa descontada, pero como regla general, el enfoque hace que sentido. Si cree que las recompensas dentro de cinco pasos de tiempo tienen poco que ver con las decisiones que se toman ahora, Œ≥=0.8 podr√≠a ser una tasa de descuento adecuada.  

Con suerte, este art√≠culo proporcion√≥ algo de claridad sobre el tema de las tasas de descuento, pero solo rasc√≥ la superficie. Se ha sugerido que todo el concepto es defectuoso para las aproximaciones de funciones (que normalmente usamos en RL), y que las recompensas promedio son una mejor m√©trica que las recompensas con descuento. Otro enfoque interesante es abandonar la noci√≥n de una tasa de descuento fija y, en su lugar, trabajar con tasas dependientes del estado. Tenga en cuenta las preferencias humanas y se abre un mundo completamente nuevo.

